# Azure VoiceLive Configuration
# Using AI Foundry resource endpoint (must be in avatar-supported region)
# Avatar regions: Southeast Asia, West US 2, East US 2, West Europe, North Europe, Sweden Central, South Central US
AZURE_VOICELIVE_ENDPOINT=wss://your-ai-foundry-resource.cognitiveservices.azure.com
AZURE_VOICELIVE_API_KEY=your-api-key-here
VOICELIVE_MODEL=gpt-4.1-mini

# Set to true to use Azure DefaultAzureCredential instead of API key
USE_TOKEN_CREDENTIAL=false

# Avatar Configuration
# Video avatars (lisa, max, jenny, guy): require style, no base model, resolution 1280x720
# Photo avatars (Sakura, Matteo, Harry, Meg, Gabrielle): require base model, no style, resolution 512x512
# Custom avatars: set AVATAR_CUSTOMIZED=true, AVATAR_BASE_MODEL=vasa-1, and use your custom avatar name
AVATAR_CHARACTER=Beatriz
AVATAR_STYLE=
AVATAR_CUSTOMIZED=true
AVATAR_BASE_MODEL=vasa-1     # Required for photo/custom avatars (leave empty for video avatars)

# Video Settings
AVATAR_VIDEO_BITRATE=2000000
AVATAR_VIDEO_CODEC=h264

# Voice Settings (use multilingual voice for multi-language support)
# Options: en-US-AvaMultilingualNeural (standard), en-US-Ava:DragonHDLatestNeural (HD quality)
VOICE_NAME=en-US-Ava:DragonHDLatestNeural

# Per-Language Voice Mappings
# Dynamic voice switching uses native voices for each detected language
# See: https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=tts
VOICE_MAPPING_EN=en-US-JennyNeural
VOICE_MAPPING_ZH=zh-CN-XiaoxiaoNeural
VOICE_MAPPING_ZH_HK=zh-HK-HiuMaanNeural
VOICE_MAPPING_MS=ms-MY-YasminNeural
VOICE_MAPPING_TA=ta-IN-PallaviNeural

# Input language detection (comma-separated for multi-language auto-detection)
# Supported language codes:
#   en     - English
#   zh     - Mandarin Chinese
#   zh-HK  - Cantonese (粤语/廣東話)
#   ms     - Malay
#   ta     - Tamil
# Note: Hokkien and Teochew are not supported by Azure Speech Services
INPUT_LANGUAGES=en,zh,zh-HK,ms,ta

# Assistant Instructions (keep responses short for voice interaction)
ASSISTANT_INSTRUCTIONS=You are a helpful AI voice assistant. Keep responses SHORT - maximum 2 sentences. Be concise and conversational.

# Maximum tokens for assistant response
MAX_RESPONSE_TOKENS=100

# Transcription Settings
# Model: whisper-1, gpt-4o-transcribe, gpt-4o-mini-transcribe, azure-speech
# azure-speech recommended for multilingual (supports Cantonese, phrase lists)
TRANSCRIPTION_MODEL=azure-speech

# Phrase list for domain-specific terms (comma-separated)
# Improves recognition of specific words like project names, acronyms
PHRASE_LIST=Digital Think Tank,DTT,NUHS,Bot-NUHS,Russell-GPT

# VAD prefix padding in ms (audio captured before speech starts)
# Increase if first words are being missed (default: 400)
VAD_PREFIX_PADDING_MS=400

# Turn-based mode: When true, VAD doesn't auto-trigger response; response waits for context
# RECOMMENDED: Set to true when using Foundry Agent to ensure RAG context is always available
# In live voice mode (false), responses may start before context retrieval completes
TURN_BASED_MODE=false

# Azure AI Foundry Agent Configuration (for RAG-augmented responses)
# Enable Foundry Agent for knowledge retrieval
FOUNDRY_AGENT_ENABLED=false

# Full AI Foundry project endpoint (required if FOUNDRY_AGENT_ENABLED=true)
# Format: https://<instance>.services.ai.azure.com/api/projects/<project-name>
# Find this in Azure AI Foundry portal > Your Project > Overview
FOUNDRY_ENDPOINT=

# Pre-created agent ID (required if FOUNDRY_AGENT_ENABLED=true)
# Create agent in Azure AI Foundry portal with file_search tool and your knowledge documents
FOUNDRY_AGENT_ID=
